---
title: "HW 4 extra credit"
author: "Ashley Miller"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(brms)
library(skimr)
library(bayesplot)
library(posterior)
library(rstan)
```

```{r load, echo=FALSE}
coff <- read.csv("sales-ds6040.csv")
```

```{r}
fit <- brm(
  sales ~ con * food + neur * food + store,
  data = coff,
  family = gaussian(),
  prior = c(
    set_prior("normal(0, 5)", class = "b"),
    set_prior("normal(0, 5)", class = "Intercept")
  )
)
summary(fit)
```
```{r param, echo=FALSE}
# Extract samples from the brms model
samples <- as.array(fit)
# Posterior distributions as area plots
mcmc_areas(samples, pars = c("b_con", "b_neur", "b_food", "b_store", "Intercept", "b_con:food", "b_food:neur"), prob = 0.95, area_method =  "scaled height")

```

In this exercise, we can see that the effects of nearly all the emotional quality coefficients are now significantly affecting the model, while the store coefficient minimally affects it. This shows that the information contained in the random intercept and slope effects is now being masked by the other predictors in a way that belies the reality (or at least the reality as we have modeled it in the other exercise).

```{r}
stan_data <- list(
  N = nrow(coff),
  J = length(unique(coff$store)),
  store = as.integer(factor(coff$store)),
  sales = coff$sales,
  con = coff$con,
  food = coff$food,
  neur = coff$neur
)

# Fit the model
fit <- stan(
  file = "barstucks.stan", # Path to the Stan model file
  data = stan_data,
  iter = 2000, 
  chains = 4
)

# Print summary of the model
print(fit)
```
```{r}
samples = extract(fit)

# Convert the samples to an array format
posterior_array <- as.array(fit)

# Plot posterior distributions for the fixed effects
mcmc_areas(posterior_array, pars = c("beta_0", "beta_con", "beta_food", "beta_con_food", "beta_neur", "beta_neur_food"),
           prob = 0.95) + # 95% credible intervals
  labs(title = "Posterior Distributions with 95% Credible Intervals",
       x = "Parameter Value",
       y = "Density")
```
```{r}
# Extract random effects samples
u_samples <- samples$u  # Array of dimension [4000, 20, 2]

# Summarize the random effects (intercepts and slopes)
summary_u_0 <- apply(u_samples[, , 1], 2, function(x) {
  data.frame(
    mean = mean(x),
    lower = quantile(x, 0.025),
    upper = quantile(x, 0.975)
  )
}) %>% bind_rows(.id = "store")

summary_u_food <- apply(u_samples[, , 2], 2, function(x) {
  data.frame(
    mean = mean(x),
    lower = quantile(x, 0.025),
    upper = quantile(x, 0.975)
  )
}) %>% bind_rows(.id = "store")

# Combine the summaries
summary_random_effects <- bind_rows(
  summary_u_0 %>% mutate(effect = "Intercept"),
  summary_u_food %>% mutate(effect = "Slope (Food)")
)
summary_random_effects <- summary_random_effects %>% mutate(store = factor(store, levels = unique(store)))

# Forest plot for random intercepts and slopes
ggplot(summary_random_effects, aes(x = store, y = mean)) +
  geom_point() +
  geom_errorbar(aes(ymin = lower, ymax = upper)) +
  facet_wrap(~ effect, scales = "free_x") +
  labs(title = "Forest Plots for Group-Level Effects",
       x = "Estimate",
       y = "Store") +
  theme_minimal() +
  theme(axis.text.y = element_text(angle = 0, hjust = 1, size = 8), 
        strip.text = element_text(size = 12))
```
This method found 13, 14, 15, and 17 to be the stores which deviated from the average significantly after controlling for fixed effects. By contrast, the food offset only significantly deviated in stores 3, 9, and 13. Since these are all 1 indexed they correspond to stores with id's 1 less than what is displayed. They differ significantly from the other methods estimates, though store 15 (14) remains a standout.
One thing that is different is that you must specify the prior tau for the multivariate normal distribution. In brms, you can specify the cholesky factor and Sigma, though I still am a bit unsure what effect specifying Sigma has, since in the prior summary it doesn't show it is assigned to any of the predictors (by contrast the cholesky factor shows it is assigned to the group level food effect). I was also under the impression you could represent Sigma with the cholesky lower triangular matrix multiplied by its transpose. I made sure to specify sigma in brms though, since the student t distribution gets assigned by default if you don't.



